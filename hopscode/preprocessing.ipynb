{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Built on fMoW Baseline: https://github.com/fMoW/baseline\n",
    "'''\n",
    "import hops.hdfs as hdfs\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import errno\n",
    "import numpy as np\n",
    "import string\n",
    "import dateutil.parser as dparser\n",
    "#from PIL import Image\n",
    "#from sklearn.utils import class_weight\n",
    "from keras.preprocessing import image\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "params = {\n",
    "    \"num_workers\":  cpu_count(),\n",
    "\n",
    "    \"use_metadata\":  True,\n",
    "\n",
    "    \"batch_size_cnn\":  128,\n",
    "    \"batch_size_lstm\":  512,\n",
    "    \"batch_size_eval\":  128,\n",
    "    \"metadata_length\":  45,\n",
    "    \"num_channels\":  3,\n",
    "    \"cnn_last_layer_length\":  4096,\n",
    "    \"cnn_lstm_layer_length\":  2208,\n",
    "\n",
    "    \"target_img_size\":  (224,224),\n",
    "\n",
    "    \"image_format\":  'jpg',\n",
    "\n",
    "    \"train_cnn\":  False,\n",
    "    \"generate_cnn_codes\":  False,\n",
    "    \"train_lstm\":  False,\n",
    "    \"test_cnn\":  False,\n",
    "    \"test_lstm\":  False,\n",
    "\n",
    "    #LEARNING PARAMS,\n",
    "    \"cnn_adam_learning_rate\":  1e-4,\n",
    "    \"cnn_adam_loss\":  'categorical_crossentropy',\n",
    "    \"cnn_epochs\":  50,\n",
    "\n",
    "    \"lstm_adam_learning_rate\":  1e-4,\n",
    "    \"lstm_epochs\":  100,\n",
    "    \"lstm_loss\":  'categorical_crossentropy',\n",
    "}\n",
    "\n",
    "#DIRECTORIES AND FILES,\n",
    "params[\"directories\"] = dict()\n",
    "params[\"directories\"]['dataset'] = '../../fmow_dataset'\n",
    "params[\"directories\"]['input'] = os.path.join('..', 'data', 'input')\n",
    "params[\"directories\"]['output'] = os.path.join('..', 'data', 'output')\n",
    "params[\"directories\"]['working'] = os.path.join('..', 'data', 'working')\n",
    "params[\"directories\"]['train_data'] = os.path.join(params[\"directories\"]['input'], 'train_data')\n",
    "params[\"directories\"]['test_data'] = os.path.join(params[\"directories\"]['input'], 'test_data')\n",
    "params[\"directories\"]['cnn_models'] = os.path.join(params[\"directories\"]['working'], 'cnn_models')\n",
    "params[\"directories\"]['lstm_models'] = os.path.join(params[\"directories\"]['working'], 'lstm_models')\n",
    "params[\"directories\"]['predictions'] = os.path.join(params[\"directories\"]['output'], 'predictions')\n",
    "params[\"directories\"]['cnn_checkpoint_weights'] = os.path.join(params[\"directories\"]['working'], 'cnn_checkpoint_weights')\n",
    "params[\"directories\"]['lstm_checkpoint_weights'] = os.path.join(params[\"directories\"]['working'], 'lstm_checkpoint_weights')\n",
    "params[\"directories\"]['cnn_codes'] = os.path.join(params[\"directories\"]['working'], 'cnn_codes')\n",
    "\n",
    "params[\"files\"] = {}\n",
    "params[\"files\"]['training_struct'] = os.path.join(params[\"directories\"]['working'], 'training_struct.json')\n",
    "params[\"files\"]['test_struct'] = os.path.join(params[\"directories\"]['working'], 'test_struct.json')\n",
    "params[\"files\"]['dataset_stats'] = os.path.join(params[\"directories\"]['working'], 'dataset_stats.json')\n",
    "params[\"files\"]['class_weight'] = os.path.join(params[\"directories\"]['working'], 'class_weights.json')\n",
    "\n",
    "\n",
    "\n",
    "params[\"category_names\"] = ['false_detection', 'airport', 'airport_hangar', 'airport_terminal', 'amusement_park', 'aquaculture', 'archaeological_site', 'barn', 'border_checkpoint', 'burial_site', 'car_dealership', 'construction_site', 'crop_field', 'dam', 'debris_or_rubble', 'educational_institution', 'electric_substation', 'factory_or_powerplant', 'fire_station', 'flooded_road', 'fountain', 'gas_station', 'golf_course', 'ground_transportation_station', 'helipad', 'hospital', 'interchange', 'lake_or_pond', 'lighthouse', 'military_facility', 'multi-unit_residential', 'nuclear_powerplant', 'office_building', 'oil_or_gas_facility', 'park', 'parking_lot_or_garage', 'place_of_worship', 'police_station', 'port', 'prison', 'race_track', 'railway_bridge', 'recreational_facility', 'impoverished_settlement', 'road_bridge', 'runway', 'shipyard', 'shopping_mall', 'single-unit_residential', 'smokestack', 'solar_farm', 'space_facility', 'stadium', 'storage_tank','surface_mine', 'swimming_pool', 'toll_booth', 'tower', 'tunnel_opening', 'waste_disposal', 'water_treatment_facility', 'wind_farm', 'zoo'],\n",
    "\n",
    "params[\"num_labels\"] = len(params[\"category_names\"]),\n",
    "#for directory in params[\"directories\"].values():\n",
    "#    if not os.path.isdir(directory):\n",
    "#        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queuing sequences in: train\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-5248ba551012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadataStats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_stats'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     '''\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-5248ba551012>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Queuing sequences in: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurrDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#for root, dirs, files in tqdm(os.walk(os.path.join(params.directories['dataset'], currDir))):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;31m#if len(files) > 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m#    slashes = [i for i,ltr in enumerate(root) if ltr == '/']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "def prepare_data(params):\n",
    "    \"\"\"\n",
    "    Saves sub images, converts metadata to feature vectors and saves in JSON files, \n",
    "    calculates dataset statistics, and keeps track of saved files so they can be loaded as batches\n",
    "    while training the CNN.\n",
    "    :param params: global parameters, used to find location of the dataset and json file\n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    # suppress decompression bomb warnings for Pillow\n",
    "    warnings.simplefilter('ignore', Image.DecompressionBombWarning)\n",
    "\n",
    "    walkDirs = ['train', 'val', 'test']\n",
    "\n",
    "    executor = ProcessPoolExecutor(max_workers=params.num_workers)\n",
    "    futures = []\n",
    "    paramsDict = vars(params)\n",
    "    keysToKeep = ['image_format', 'target_img_size', 'metadata_length', 'category_names']\n",
    "    paramsDict = {keepKey: paramsDict[keepKey] for keepKey in keysToKeep}\n",
    "    \n",
    "    for currDir in walkDirs:\n",
    "        isTrain = (currDir == 'train') or (currDir == 'val')\n",
    "        if isTrain:\n",
    "            outDir = params.directories['train_data']\n",
    "        else:\n",
    "            outDir = params.directories['test_data']\n",
    "\n",
    "        print('Queuing sequences in: ' + currDir)\n",
    "        #for root, dirs, files in tqdm(os.walk(os.path.join(params.directories['dataset'], currDir))):\n",
    "        for file in hdfs.get_fs().walk(os.path.join(params.directories['dataset'], currDir)):\n",
    "            if len(files) > 0:\n",
    "                slashes = [i for i,ltr in enumerate(root) if ltr == '/']\n",
    "                        \n",
    "            for file in files:\n",
    "                if file.endswith('_rgb.json'): #skip _msrgb images\n",
    "                    task = partial(_process_file, file, slashes, root, isTrain, outDir, paramsDict)\n",
    "                    futures.append(executor.submit(task))\n",
    "    '''\n",
    "    print('Wait for all preprocessing tasks to complete...')\n",
    "    results = []\n",
    "    [results.extend(future.result()) for future in tqdm(futures)]\n",
    "    allTrainFeatures = [np.array(r[0]) for r in results if r[0] is not None]\n",
    "    \n",
    "    metadataTrainSum = np.zeros(params.metadata_length)bcpu\n",
    "    for features in allTrainFeatures:\n",
    "        metadataTrainSum += features\n",
    "\n",
    "    trainingData = [r[1] for r in results if r[1] is not None]\n",
    "    trainCount = len(trainingData)\n",
    "    testData = [r[2] for r in results if r[2] is not None]\n",
    "\n",
    "    # Shutdown the executor and free resources\n",
    "    executor.shutdown()\n",
    "\n",
    "    metadataMean = metadataTrainSum / trainCount\n",
    "    metadataMax = np.zeros(params.metadata_length)\n",
    "    for currFeat in allTrainFeatures:\n",
    "        currFeat = currFeat - metadataMean\n",
    "        for i in range(params.metadata_length):\n",
    "            if abs(currFeat[i]) > metadataMax[i]:\n",
    "                metadataMax[i] = abs(currFeat[i])\n",
    "    for i in range(params.metadata_length):\n",
    "        if metadataMax[i] == 0:\n",
    "            metadataMax[i] = 1.0\n",
    "    metadataStats = {}\n",
    "    metadataStats['metadata_mean'] = metadataMean.tolist()\n",
    "    metadataStats['metadata_max'] = metadataMax.tolist()\n",
    "    json.dump(testData, open(params.files['test_struct'], 'w'))\n",
    "    json.dump(trainingData, open(params.files['training_struct'], 'w'))\n",
    "    json.dump(metadataStats, open(params.files['dataset_stats'], 'w'))    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prepare_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _process_file(file, slashes, root, isTrain, outDir, params):\n",
    "    \"\"\"\n",
    "    Helper for prepare_data that actually loads and resizes each image and computes\n",
    "    feature vectors. This function is designed to be called in parallel for each file\n",
    "    :param file: file to process\n",
    "    :param slashes: location of slashes from root walk path\n",
    "    :param root: root walk path\n",
    "    :param isTrain: flag on whether or not the current file is from the train set\n",
    "    :param outDir: output directory for processed data\n",
    "    :param params: dict of the global parameters with only the necessary fields\n",
    "    :return (allFeatures, allTrainResults, allTestResults)\n",
    "    \"\"\"\n",
    "    noResult = [(None, None, None)]\n",
    "    baseName = file[:-5]\n",
    "\n",
    "    imgFile = baseName + '.' + params['image_format']\n",
    "\n",
    "    if not os.path.isfile(os.path.join(root, imgFile)):\n",
    "        return noResult\n",
    "\n",
    "    try:\n",
    "        img = image.load_img(os.path.join(root, imgFile))\n",
    "        img = image.img_to_array(img)\n",
    "    except:\n",
    "        return noResult\n",
    "\n",
    "    jsonData = json.load(open(os.path.join(root, file)))\n",
    "    if not isinstance(jsonData['bounding_boxes'], list):\n",
    "        jsonData['bounding_boxes'] = [jsonData['bounding_boxes']]\n",
    "\n",
    "    allResults = []\n",
    "    for bb in jsonData['bounding_boxes']:\n",
    "        if isTrain:\n",
    "            category = bb['category']\n",
    "        box = bb['box']\n",
    "\n",
    "        outBaseName = '%d' % bb['ID']\n",
    "        if isTrain:\n",
    "            outBaseName = ('%s_' % category) + outBaseName\n",
    "\n",
    "        if isTrain:\n",
    "            currOut = os.path.join(outDir, root[slashes[-3] + 1:], outBaseName)\n",
    "        else:\n",
    "            currOut = os.path.join(outDir, root[slashes[-2] + 1:], outBaseName)\n",
    "\n",
    "        if not os.path.isdir(currOut):\n",
    "            try:\n",
    "                os.makedirs(currOut)\n",
    "            except OSError as e:\n",
    "                if e.errno == errno.EEXIST:\n",
    "                    pass\n",
    "\n",
    "        featuresPath = os.path.join(currOut, baseName + '_features.json')\n",
    "        imgPath = os.path.join(currOut, imgFile)\n",
    "\n",
    "        # don't train on tiny boxes\n",
    "        if box[2] <= 2 or box[3] <= 2:\n",
    "            continue\n",
    "\n",
    "        # train with context around box\n",
    "        \n",
    "        contextMultWidth = 0.15\n",
    "        contextMultHeight = 0.15\n",
    "        \n",
    "        wRatio = float(box[2]) / img.shape[0]\n",
    "        hRatio = float(box[3]) / img.shape[1]\n",
    "        \n",
    "        if wRatio < 0.5 and wRatio >= 0.4:\n",
    "            contextMultWidth = 0.2\n",
    "        if wRatio < 0.4 and wRatio >= 0.3:\n",
    "            contextMultWidth = 0.3\n",
    "        if wRatio < 0.3 and wRatio >= 0.2:\n",
    "            contextMultWidth = 0.5\n",
    "        if wRatio < 0.2 and wRatio >= 0.1:\n",
    "            contextMultWidth = 1\n",
    "        if wRatio < 0.1:\n",
    "            contextMultWidth = 2\n",
    "            \n",
    "        if hRatio < 0.5 and hRatio >= 0.4:\n",
    "            contextMultHeight = 0.2\n",
    "        if hRatio < 0.4 and hRatio >= 0.3:\n",
    "            contextMultHeight = 0.3\n",
    "        if hRatio < 0.3 and hRatio >= 0.2:\n",
    "            contextMultHeight = 0.5\n",
    "        if hRatio < 0.2 and hRatio >= 0.1:\n",
    "            contextMultHeight = 1\n",
    "        if hRatio < 0.1:\n",
    "            contextMultHeight = 2\n",
    "        \n",
    "        \n",
    "        widthBuffer = int((box[2] * contextMultWidth) / 2.0)\n",
    "        heightBuffer = int((box[3] * contextMultHeight) / 2.0)\n",
    "\n",
    "        r1 = box[1] - heightBuffer\n",
    "        r2 = box[1] + box[3] + heightBuffer\n",
    "        c1 = box[0] - widthBuffer\n",
    "        c2 = box[0] + box[2] + widthBuffer\n",
    "\n",
    "        if r1 < 0:\n",
    "            r1 = 0\n",
    "        if r2 > img.shape[0]:\n",
    "            r2 = img.shape[0]\n",
    "        if c1 < 0:\n",
    "            c1 = 0\n",
    "        if c2 > img.shape[1]:\n",
    "            c2 = img.shape[1]\n",
    "\n",
    "        if r1 >= r2 or c1 >= c2:\n",
    "            continue\n",
    "\n",
    "        subImg = img[r1:r2, c1:c2, :]\n",
    "        subImg = image.array_to_img(subImg)\n",
    "        subImg = subImg.resize(params['target_img_size'])\n",
    "        subImg.save(imgPath)\n",
    "\n",
    "        features = json_to_feature_vector(params, jsonData, bb)\n",
    "        features = features.tolist()\n",
    "\n",
    "        json.dump(features, open(featuresPath, 'w'))\n",
    "        \n",
    "\n",
    "        if isTrain:\n",
    "            allResults.append((features, {\"features_path\": featuresPath, \"img_path\": imgPath, \"category\": params['category_names'].index(category)}, None))\n",
    "        else:\n",
    "            allResults.append((None, None, {\"features_path\": featuresPath, \"img_path\": imgPath}))\n",
    "\n",
    "    return allResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_folder = \"hdfs:///Projects/labs/fMoW/train\"\n",
    "\n",
    "count = 0\n",
    "\n",
    "files = hdfs.get_fs().walk(train_folder)\n",
    "\n",
    "for file in hdfs.get_fs().walk(train_folder):\n",
    "    if file['name'].endswith('_rgb.jpg'):\n",
    "        count = count + 1\n",
    "        if (count % 1000 == 0):\n",
    "            print(count)\n",
    "    \n",
    "with hdfs.hdfs.open(temp_img, \"r\") as curr_file:\n",
    "    img_data = curr_file.read()\n",
    "    img = cv2.imdecode(np.fromstring(img_data, dtype=np.uint8), -1)\n",
    "    print(img.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-fmow_grus__tiol0000",
   "language": "python",
   "name": "python-fmow_grus__tiol0000"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
